{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sharon Kim\n",
    "SBA 345: Apache Spark (PySpark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section One - CompanyABC Stock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.1\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "\n",
    "spark_df = spark.read.load(\"C:\\\\Users\\\\SGKsk\\\\Downloads\\\\CompanyABC_stock.csv\", format=\"csv\", header = True,inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n",
      "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|summary|              Open|             High|              Low|            Close|           Volume|        Adj Close|\n",
      "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "|  count|              1258|             1258|             1258|             1258|             1258|             1258|\n",
      "|   mean| 72.35785375357709|72.83938807631165| 71.9186009594594|72.38844998012726|8222093.481717011|67.23883848728146|\n",
      "| stddev|  6.76809024470826|6.768186808159218|6.744075756255496|6.756859163732991|  4519780.8431556|6.722609449996857|\n",
      "|    min|56.389998999999996|        57.060001|        56.299999|        56.419998|          2094900|        50.363689|\n",
      "|    max|         90.800003|        90.970001|            89.25|        90.470001|         80898100|84.91421600000001|\n",
      "+-------+------------------+-----------------+-----------------+-----------------+-----------------+-----------------+\n",
      "\n",
      "+----------+------------------+------------------+------------------+------------------+--------+------------------+\n",
      "|      Date|              Open|              High|               Low|             Close|  Volume|         Adj Close|\n",
      "+----------+------------------+------------------+------------------+------------------+--------+------------------+\n",
      "|2012-01-03|         59.970001|         61.060001|         59.869999|         60.330002|12668800|52.619234999999996|\n",
      "|2012-01-04|60.209998999999996|         60.349998|         59.470001|59.709998999999996| 9593300|         52.078475|\n",
      "|2012-01-05|         59.349998|         59.619999|         58.369999|         59.419998|12768200|         51.825539|\n",
      "|2012-01-06|         59.419998|         59.450001|         58.869999|              59.0| 8069400|          51.45922|\n",
      "|2012-01-09|         59.029999|         59.549999|         58.919998|             59.18| 6679300|51.616215000000004|\n",
      "|2012-01-10|             59.43|59.709998999999996|             58.98|59.040001000000004| 6907300|         51.494109|\n",
      "|2012-01-11|         59.060001|         59.529999|59.040001000000004|         59.400002| 6365600|         51.808098|\n",
      "|2012-01-12|59.790001000000004|              60.0|         59.400002|              59.5| 7236400|51.895315999999994|\n",
      "|2012-01-13|             59.18|59.610001000000004|59.009997999999996|59.540001000000004| 7729300|51.930203999999996|\n",
      "|2012-01-17|         59.869999|60.110001000000004|             59.52|         59.849998| 8500000|         52.200581|\n",
      "|2012-01-18|59.790001000000004|         60.029999|         59.650002|60.009997999999996| 5911400|         52.340131|\n",
      "|2012-01-19|             59.93|             60.73|             59.75|60.610001000000004| 9234600|         52.863447|\n",
      "|2012-01-20|             60.75|             61.25|         60.669998|61.009997999999996|10378800|53.212320999999996|\n",
      "|2012-01-23|         60.810001|             60.98|60.509997999999996|             60.91| 7134100|         53.125104|\n",
      "|2012-01-24|             60.75|              62.0|             60.75|61.389998999999996| 7362800| 53.54375400000001|\n",
      "|2012-01-25|             61.18|61.610001000000004|61.040001000000004|         61.470001| 5915800| 53.61353100000001|\n",
      "|2012-01-26|         61.799999|             61.84|             60.77|         60.970001| 7436200|         53.177436|\n",
      "|2012-01-27|60.860001000000004|         61.119999|60.540001000000004|60.709998999999996| 6287300|         52.950665|\n",
      "|2012-01-30|         60.470001|             61.32|         60.349998|         61.299999| 7636900|53.465256999999994|\n",
      "|2012-01-31|         61.529999|             61.57|         60.580002|61.360001000000004| 9761500|53.517590000000006|\n",
      "+----------+------------------+------------------+------------------+------------------+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.columns\n",
    "spark_df.printSchema()\n",
    "spark_df.describe().show()\n",
    "spark_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Date=datetime.date(2012, 1, 3), Open=59.970001, High=61.060001, Low=59.869999, Close=60.330002, Volume=12668800, Adj Close=52.619234999999996),\n",
       " Row(Date=datetime.date(2012, 1, 4), Open=60.209998999999996, High=60.349998, Low=59.470001, Close=59.709998999999996, Volume=9593300, Adj Close=52.078475),\n",
       " Row(Date=datetime.date(2012, 1, 5), Open=59.349998, High=59.619999, Low=58.369999, Close=59.419998, Volume=12768200, Adj Close=51.825539),\n",
       " Row(Date=datetime.date(2012, 1, 6), Open=59.419998, High=59.450001, Low=58.869999, Close=59.0, Volume=8069400, Adj Close=51.45922),\n",
       " Row(Date=datetime.date(2012, 1, 9), Open=59.029999, High=59.549999, Low=58.919998, Close=59.18, Volume=6679300, Adj Close=51.616215000000004)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.2 print first five rows\n",
    "\n",
    "spark_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            HV Ratio|\n",
      "+--------------------+\n",
      "|4.819714653321546E-6|\n",
      "|6.290848613094555E-6|\n",
      "|4.669412994783916E-6|\n",
      "|7.367338463826307E-6|\n",
      "|8.915604778943901E-6|\n",
      "|8.644477436914568E-6|\n",
      "|9.351828421515645E-6|\n",
      "| 8.29141562102703E-6|\n",
      "|7.712212102001476E-6|\n",
      "|7.071764823529412E-6|\n",
      "|1.015495466386981E-5|\n",
      "|6.576354146362592...|\n",
      "| 5.90145296180676E-6|\n",
      "|8.547679455011844E-6|\n",
      "|8.420709512685392E-6|\n",
      "|1.041448341728929...|\n",
      "|8.316075414862431E-6|\n",
      "|9.721183814992126E-6|\n",
      "|8.029436027707578E-6|\n",
      "|6.307432259386365E-6|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1.3: Create a new Dataframe column called “HV Ratio,” which will stimulate the ratio of the High price versus the total Volume of stock that was traded for a day. \n",
    "\n",
    "#new_column_df = spark_df.withColumn('HV Ratio', spark_df.High / spark_df.Volume)\n",
    "#new_column_df.show()\n",
    "\n",
    "new_column_df = spark_df.withColumn('HV Ratio', spark_df['High'] / spark_df['Volume']).select('HV Ratio').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+\n",
      "|             High|      Date|\n",
      "+-----------------+----------+\n",
      "|        90.970001|2015-01-13|\n",
      "|90.66999799999999|2015-01-08|\n",
      "|        90.389999|2015-01-09|\n",
      "|        90.309998|2015-01-12|\n",
      "|        89.260002|2015-01-23|\n",
      "|        89.160004|2015-01-26|\n",
      "|            88.68|2015-01-07|\n",
      "|        88.519997|2015-01-14|\n",
      "|        88.459999|2015-01-27|\n",
      "|        88.400002|2015-01-22|\n",
      "|        88.230003|2015-01-28|\n",
      "|        88.089996|2014-11-28|\n",
      "|             88.0|2015-02-06|\n",
      "|        87.779999|2015-01-15|\n",
      "|        87.720001|2015-01-29|\n",
      "|        87.699997|2015-01-20|\n",
      "|        87.459999|2015-01-16|\n",
      "|        87.440002|2014-12-31|\n",
      "|        87.410004|2015-02-10|\n",
      "|        87.360001|2015-01-30|\n",
      "+-----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1.4: Find out on what day the stock price was the highest. (Hint: use the High column.)\n",
    "\n",
    "spark_df.select(\"High\", \"Date\").sort(spark_df['High'].desc()).show()\n",
    "\n",
    "#The stock price was the highest on Jan 13, 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       avg(Close)|\n",
      "+-----------------+\n",
      "|72.38844998012726|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1.5: What is the average (mean) closing price? (Hint: Use the Close column.)\n",
    "\n",
    "from pyspark.sql.functions import mean,avg,max,min\n",
    "\n",
    "spark_df.select(mean(\"Close\")).show()\n",
    "\n",
    "#The average closing price is ~$72.39."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|max(Volume)|min(Volume)|\n",
      "+-----------+-----------+\n",
      "|   80898100|    2094900|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#1.6: What is the maximum and minimum volume of stock traded? (Hint: Use the Volume column).\n",
    "\n",
    "spark_df.select(max(\"Volume\"),min(\"Volume\")).show()\n",
    "\n",
    "#The maximum and minimum volume of stock traded is 80898100 and 2094900, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "397"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.7: For how many days was the closing value less than 70 dollars? (Hint: Use the count() method.)\n",
    "\n",
    "#spark_70_df = spark_df.select('Date', 'Close').filter(spark_df['Close'] < 70).show(400)\n",
    "spark_df.select('Date', 'Close').filter(spark_df['Close'] < 70).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.141494435612083%\n"
     ]
    }
   ],
   "source": [
    "#1.8: What percentage of the time were the High greater than 80 dollars? \n",
    "\n",
    "#spark_df.count() #1258\n",
    "#spark_df.select('High').filter(spark_df['High'] > 80).count() #115\n",
    "#print(str(115/1258 * 100) + (\"%\"))\n",
    "print(str((spark_df.select('High').filter(spark_df['High'] > 80).count() / spark_df.count()) * 100) + (\"%\"))\n",
    "\n",
    "#The High was greater than $80 ~9.14% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.9: Create a database named CompanyABC_DB using SQL (Workbench). \n",
    "\n",
    "#done in SQL Workbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.10: Load/Write CompanyABC stock.csv file data into CompanyABC_DB database from SparkSQL Dataframe. You can specify any table name for that file.\n",
    "\n",
    "#df = spark.createDataFrame(data = \"C:\\\\Users\\\\SGKsk\\\\Downloads\\\\CompanyABC_stock.csv\")\n",
    "\n",
    "df = spark.read.load(\"C:\\\\Users\\\\SGKsk\\\\Downloads\\\\CompanyABC_stock.csv\", format=\"csv\", header = True ,inferSchema = True)\n",
    "\n",
    "\n",
    "df.write.format(\"jdbc\") \\\n",
    " .mode(\"append\") \\\n",
    " .option(\"url\", \"jdbc:mysql://localhost:3306/companyabc_db\") \\\n",
    " .option(\"dbtable\", \"companyabc_db.Table_forSpark\") \\\n",
    " .option(\"user\", \"root\") \\\n",
    " .option(\"password\", \"password\") \\\n",
    " .save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section Two - CompanyABC Sales Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------------+----------+---------------+--------------------+\n",
      "|Order ID|             Product|Quantity Ordered|Price Each|     Order Date|    Purchase Address|\n",
      "+--------+--------------------+----------------+----------+---------------+--------------------+\n",
      "|  176558|USB-C Charging Cable|               2|     11.95| 4/19/2019 8:46|917 1st St, Dalla...|\n",
      "|  176559|Bose SoundSport H...|               1|     99.99| 4/7/2019 22:30|682 Chestnut St, ...|\n",
      "|  176560|        Google Phone|               1|       600|4/12/2019 14:38|669 Spruce St, Lo...|\n",
      "|  176560|    Wired Headphones|               1|     11.99|4/12/2019 14:38|669 Spruce St, Lo...|\n",
      "|  176561|    Wired Headphones|               1|     11.99| 4/30/2019 9:27|333 8th St, Los A...|\n",
      "|  176562|USB-C Charging Cable|               1|     11.95|4/29/2019 13:03|381 Wilson St, Sa...|\n",
      "|  176563|Bose SoundSport H...|               1|     99.99|  4/2/2019 7:46|668 Center St, Se...|\n",
      "|  176564|USB-C Charging Cable|               1|     11.95|4/12/2019 10:58|790 Ridge St, Atl...|\n",
      "|  176565|  Macbook Pro Laptop|               1|      1700|4/24/2019 10:38|915 Willow St, Sa...|\n",
      "|  176566|    Wired Headphones|               1|     11.99| 4/8/2019 14:05|83 7th St, Boston...|\n",
      "|  176567|        Google Phone|               1|       600|4/18/2019 17:18|444 7th St, Los A...|\n",
      "|  176568|Lightning Chargin...|               1|     14.95|4/15/2019 12:18|438 Elm St, Seatt...|\n",
      "|  176569|27in 4K Gaming Mo...|               1|    389.99|4/16/2019 19:23|657 Hill St, Dall...|\n",
      "|  176570|AA Batteries (4-p...|               1|      3.84|4/22/2019 15:09|186 12th St, Dall...|\n",
      "|  176571|Lightning Chargin...|               1|     14.95|4/19/2019 14:29|253 Johnson St, A...|\n",
      "|  176572|Apple Airpods Hea...|               1|       150| 4/4/2019 20:30|149 Dogwood St, N...|\n",
      "|  176573|USB-C Charging Cable|               1|     11.95|4/27/2019 18:41|214 Chestnut St, ...|\n",
      "|  176574|        Google Phone|               1|       600| 4/3/2019 19:42|20 Hill St, Los A...|\n",
      "|  176574|USB-C Charging Cable|               1|     11.95| 4/3/2019 19:42|20 Hill St, Los A...|\n",
      "|  176575|AAA Batteries (4-...|               1|      2.99| 4/27/2019 0:30|433 Hill St, New ...|\n",
      "+--------+--------------------+----------------+----------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2.1: Load/Read both CompanyABC sales datasets (Sales_April_2019.csv and Sales_February_2019.csv) into a single SparkSQL Dataframe ( hint read() ).\n",
    "\n",
    "#df_multiple = spark.read.load(\"C:\\\\Users\\\\SGKsk\\\\Downloads\\\\Sales_February_2019.csv\", \"C:\\\\Users\\\\SGKsk\\\\Downloads\\\\Sales_April_2019.csv\", format=\"csv\", header = True ,inferSchema = True)\n",
    "\n",
    "\n",
    "# Reading multiple CSV files into a DataFrame\n",
    "df_multiple = spark.read.csv([\n",
    " \"C:\\\\Users\\\\SGKsk\\\\Downloads\\\\Sales_February_2019.csv\",\n",
    " \"C:\\\\Users\\\\SGKsk\\\\Downloads\\\\Sales_April_2019.csv\"\n",
    " ], header = True,)\n",
    "df_multiple.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Order ID: string (nullable = true)\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Quantity Ordered: string (nullable = true)\n",
      " |-- Price Each: string (nullable = true)\n",
      " |-- Order Date: string (nullable = true)\n",
      " |-- Purchase Address: string (nullable = true)\n",
      "\n",
      "+-------+------------------+------------+-------------------+------------------+--------------+--------------------+\n",
      "|summary|          Order ID|     Product|   Quantity Ordered|        Price Each|    Order Date|    Purchase Address|\n",
      "+-------+------------------+------------+-------------------+------------------+--------------+--------------------+\n",
      "|  count|             30328|       30328|              30328|             30328|         30328|               30328|\n",
      "|   mean|173816.63355904212|        NULL| 1.1239966969446737| 183.7621383980421|          NULL|                NULL|\n",
      "| stddev|14902.179833784816|        NULL|0.43431295924749175|328.79347229907665|          NULL|                NULL|\n",
      "|    min|            150502|20in Monitor|                  1|            109.99|02/01/19 01:51|1 14th St, New Yo...|\n",
      "|    max|          Order ID|      iPhone|   Quantity Ordered|        Price Each|    Order Date|    Purchase Address|\n",
      "+-------+------------------+------------+-------------------+------------------+--------------+--------------------+\n",
      "\n",
      "+--------+--------------------+----------------+----------+---------------+--------------------+\n",
      "|Order ID|             Product|Quantity Ordered|Price Each|     Order Date|    Purchase Address|\n",
      "+--------+--------------------+----------------+----------+---------------+--------------------+\n",
      "|  176558|USB-C Charging Cable|               2|     11.95| 4/19/2019 8:46|917 1st St, Dalla...|\n",
      "|  176559|Bose SoundSport H...|               1|     99.99| 4/7/2019 22:30|682 Chestnut St, ...|\n",
      "|  176560|        Google Phone|               1|       600|4/12/2019 14:38|669 Spruce St, Lo...|\n",
      "|  176560|    Wired Headphones|               1|     11.99|4/12/2019 14:38|669 Spruce St, Lo...|\n",
      "|  176561|    Wired Headphones|               1|     11.99| 4/30/2019 9:27|333 8th St, Los A...|\n",
      "|  176562|USB-C Charging Cable|               1|     11.95|4/29/2019 13:03|381 Wilson St, Sa...|\n",
      "|  176563|Bose SoundSport H...|               1|     99.99|  4/2/2019 7:46|668 Center St, Se...|\n",
      "|  176564|USB-C Charging Cable|               1|     11.95|4/12/2019 10:58|790 Ridge St, Atl...|\n",
      "|  176565|  Macbook Pro Laptop|               1|      1700|4/24/2019 10:38|915 Willow St, Sa...|\n",
      "|  176566|    Wired Headphones|               1|     11.99| 4/8/2019 14:05|83 7th St, Boston...|\n",
      "|  176567|        Google Phone|               1|       600|4/18/2019 17:18|444 7th St, Los A...|\n",
      "|  176568|Lightning Chargin...|               1|     14.95|4/15/2019 12:18|438 Elm St, Seatt...|\n",
      "|  176569|27in 4K Gaming Mo...|               1|    389.99|4/16/2019 19:23|657 Hill St, Dall...|\n",
      "|  176570|AA Batteries (4-p...|               1|      3.84|4/22/2019 15:09|186 12th St, Dall...|\n",
      "|  176571|Lightning Chargin...|               1|     14.95|4/19/2019 14:29|253 Johnson St, A...|\n",
      "|  176572|Apple Airpods Hea...|               1|       150| 4/4/2019 20:30|149 Dogwood St, N...|\n",
      "|  176573|USB-C Charging Cable|               1|     11.95|4/27/2019 18:41|214 Chestnut St, ...|\n",
      "|  176574|        Google Phone|               1|       600| 4/3/2019 19:42|20 Hill St, Los A...|\n",
      "|  176574|USB-C Charging Cable|               1|     11.95| 4/3/2019 19:42|20 Hill St, Los A...|\n",
      "|  176575|AAA Batteries (4-...|               1|      2.99| 4/27/2019 0:30|433 Hill St, New ...|\n",
      "+--------+--------------------+----------------+----------+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_multiple.columns\n",
    "df_multiple.printSchema()\n",
    "df_multiple.describe().show()\n",
    "df_multiple.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------------+----------+---------------+--------------------+-----------+\n",
      "|Order ID|             Product|Quantity Ordered|Price Each|     Order Date|    Purchase Address|Total Price|\n",
      "+--------+--------------------+----------------+----------+---------------+--------------------+-----------+\n",
      "|  176558|USB-C Charging Cable|               2|     11.95| 4/19/2019 8:46|917 1st St, Dalla...|       23.9|\n",
      "|  176559|Bose SoundSport H...|               1|     99.99| 4/7/2019 22:30|682 Chestnut St, ...|      99.99|\n",
      "|  176560|        Google Phone|               1|       600|4/12/2019 14:38|669 Spruce St, Lo...|      600.0|\n",
      "|  176560|    Wired Headphones|               1|     11.99|4/12/2019 14:38|669 Spruce St, Lo...|      11.99|\n",
      "|  176561|    Wired Headphones|               1|     11.99| 4/30/2019 9:27|333 8th St, Los A...|      11.99|\n",
      "|  176562|USB-C Charging Cable|               1|     11.95|4/29/2019 13:03|381 Wilson St, Sa...|      11.95|\n",
      "|  176563|Bose SoundSport H...|               1|     99.99|  4/2/2019 7:46|668 Center St, Se...|      99.99|\n",
      "|  176564|USB-C Charging Cable|               1|     11.95|4/12/2019 10:58|790 Ridge St, Atl...|      11.95|\n",
      "|  176565|  Macbook Pro Laptop|               1|      1700|4/24/2019 10:38|915 Willow St, Sa...|     1700.0|\n",
      "|  176566|    Wired Headphones|               1|     11.99| 4/8/2019 14:05|83 7th St, Boston...|      11.99|\n",
      "|  176567|        Google Phone|               1|       600|4/18/2019 17:18|444 7th St, Los A...|      600.0|\n",
      "|  176568|Lightning Chargin...|               1|     14.95|4/15/2019 12:18|438 Elm St, Seatt...|      14.95|\n",
      "|  176569|27in 4K Gaming Mo...|               1|    389.99|4/16/2019 19:23|657 Hill St, Dall...|     389.99|\n",
      "|  176570|AA Batteries (4-p...|               1|      3.84|4/22/2019 15:09|186 12th St, Dall...|       3.84|\n",
      "|  176571|Lightning Chargin...|               1|     14.95|4/19/2019 14:29|253 Johnson St, A...|      14.95|\n",
      "|  176572|Apple Airpods Hea...|               1|       150| 4/4/2019 20:30|149 Dogwood St, N...|      150.0|\n",
      "|  176573|USB-C Charging Cable|               1|     11.95|4/27/2019 18:41|214 Chestnut St, ...|      11.95|\n",
      "|  176574|        Google Phone|               1|       600| 4/3/2019 19:42|20 Hill St, Los A...|      600.0|\n",
      "|  176574|USB-C Charging Cable|               1|     11.95| 4/3/2019 19:42|20 Hill St, Los A...|      11.95|\n",
      "|  176575|AAA Batteries (4-...|               1|      2.99| 4/27/2019 0:30|433 Hill St, New ...|       2.99|\n",
      "+--------+--------------------+----------------+----------+---------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2.2. If you use the above command on the sales dataset, you will notice that each Order has “Price Each” and “Quantity Ordered” columns, but the “Total price” is missing.  See below:\n",
    "#Now, create a new Spark Dataframe column called “Total price” and find the “Total price” of the Order for the combined sales file as shown in the screenshot\n",
    "\n",
    "#new_column_df_multiple = df_multiple.withColumn('Total Price', df_multiple['Price Each'] * df_multiple['Quantity Ordered']).select('Total Price').show()\n",
    "total_price_df = df_multiple.withColumn('Total Price', df_multiple['Price Each'] * df_multiple['Quantity Ordered'])\n",
    "total_price_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.3: Load/Write sales data from SparkSQL Dataframe into CompanyABC_DB database. You can specify any name to the table. Remember “Total price” must be recorded. Your result will look similar /closer to the screenshot below\n",
    "\n",
    "total_price_df.write.format(\"jdbc\") \\\n",
    " .mode(\"append\") \\\n",
    " .option(\"url\", \"jdbc:mysql://localhost:3306/companyabc_db\") \\\n",
    " .option(\"dbtable\", \"companyabc_db.Sales_data\") \\\n",
    " .option(\"user\", \"root\") \\\n",
    " .option(\"password\", \"password\") \\\n",
    " .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
